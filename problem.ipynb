{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=1, random_action=False, max_steps=100):\n",
    "    images = []  \n",
    "    done = False\n",
    "    truncated = False\n",
    "    state, info = env.reset()\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "    while not done and not truncated:\n",
    "        i += 1\n",
    "        if i > max_steps:\n",
    "            break\n",
    "        action = np.random.randint(4) if random_action else policy[state]\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "        if not random_action:\n",
    "            print(f\"action: {action}, state: {state}, reward: {reward}, done: {done}, truncated: {truncated}, info: {info}\")\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(video_path, video_width=500):\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "total_reward = record_video(env, None, 'frozenlake_random.mp4', fps=3, random_action=True)\n",
    "print(f\"total reward: {total_reward}\")\n",
    "show_video('frozenlake_random.mp4', video_width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FrozenLake MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeMDP:\n",
    "    def __init__(self, is_slippery):\n",
    "        self.is_slippery = is_slippery\n",
    "        self.terminal_states = np.zeros(16, dtype=int)\n",
    "        self.terminal_states[[5, 7, 11, 12, 15]] = 1\n",
    "        self.reward_fn = np.zeros(16, dtype=int)\n",
    "        self.reward_fn[15] = 1\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return self.terminal_states[state]\n",
    "    \n",
    "    def get_reward_function(self):\n",
    "        return self.reward_fn\n",
    "\n",
    "    def next_state_det(self, state, action):\n",
    "        if action == 0:    # LEFT\n",
    "            next_state = state - 1 if state % 4 != 0 else state\n",
    "        elif action == 1:  # DOWN\n",
    "            next_state = state + 4 if state // 4 != 3 else state\n",
    "        elif action == 2:  # RIGHT\n",
    "            next_state = state + 1 if state % 4 != 3 else state\n",
    "        elif action == 3:  # UP\n",
    "            next_state = state - 4 if state // 4 != 0 else state\n",
    "        else:         # WRONG ACTION\n",
    "            next_state = state\n",
    "        return next_state\n",
    "    \n",
    "    def trans_prob(self, state, action):\n",
    "        prob = np.zeros((16,), dtype=float)\n",
    "        if not self.is_slippery:\n",
    "            prob[self.next_state_det(state, action)] = 1.0\n",
    "        else:\n",
    "            prob[self.next_state_det(state, action)] += 1/3\n",
    "            prob[self.next_state_det(state, (action+1)%4)] += 1/3\n",
    "            prob[self.next_state_det(state, (action-1)%4)] += 1/3\n",
    "        return prob\n",
    "\n",
    "    def next_state_reward(self, state, action):\n",
    "        next_state_probs = self.trans_prob(state, action)\n",
    "        next_state = np.random.choice(16, p=next_state_probs)\n",
    "        reward = self.reward_fn[next_state]\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = FrozenLakeMDP(is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function of the environent\n",
    "dynamics.get_reward_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating if a given state is a terminal state (= hole or goal)\n",
    "print(dynamics.is_terminal(0), dynamics.is_terminal(7), dynamics.is_terminal(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we take action `a` in state `s`,\n",
    "# what is the probability of landing in each state?\n",
    "dynamics.trans_prob(14, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we take action `a` in state `s`, what do we get?\n",
    "# this is done through sampling the transition probability\n",
    "next_state, reward = dynamics.next_state_reward(14, 2)\n",
    "print(next_state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(dynamics, policy, gamma=0.9, num_iter=10):\n",
    "    \"\"\"\n",
    "    evaluates policy based on Iterative Policy Evaluation.\n",
    " \n",
    "    Args:\n",
    "        dynamics (FrozenLakeMDP): dynamics of the environment.\n",
    "        policy (numpy.array): policy we want to evaluate.\n",
    "        gamma (float): discount factor.\n",
    "        num_iter (int): number of iterations for the loop.\n",
    " \n",
    "    Returns:\n",
    "        numpy.array: state value function.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement Iterative Policy Evaluation algorithm\n",
    "\n",
    "    s_value_function = np.zeros(16, dtype=float)\n",
    "    return s_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = FrozenLakeMDP(is_slippery=False)\n",
    "\n",
    "# 1. go-right policy\n",
    "policy = 2 * np.ones(16, dtype=int)\n",
    "\n",
    "# 2. shortest-path policy\n",
    "policy = np.array([1, 2, 1, 0, 1, -1, 1, -1, 2, 1, 1, -1, -1, 2, 2, -1])\n",
    "\n",
    "s_value_function = policy_evaluation(dynamics, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print and analyze the state value function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_improvement(dynamics, s_value_function, gamma=0.9):\n",
    "    \"\"\"\n",
    "    obtains a policy in a greedy manner based on current state value function.\n",
    " \n",
    "    Args:\n",
    "        dynamics (FrozenLakeMDP): dynamics of the environment.\n",
    "        s_value_function (numpy.array): state value function.\n",
    "        gamma (float): discount factor.\n",
    " \n",
    "    Returns:\n",
    "        numpy.array: the greedy policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement Greedy Policy Improvement algorithm\n",
    "\n",
    "    policy = np.random.randint(0, 4, size=16)    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(dynamics, gamma=0.9, outer_iter=100, inner_iter=100):\n",
    "    \"\"\"\n",
    "    optimizes a policy based on Policy Iteration\n",
    " \n",
    "    Args:\n",
    "        dynamics (FrozenLakeMDP): dynamics of the environment.\n",
    "        gamma (float): discount factor.\n",
    "        outer_iter (int): number of iterations for the Policy Iteration loop.\n",
    "        inner_iter (int): number of iterations for the Policy Evaluation loop.\n",
    " \n",
    "    Returns:\n",
    "        numpy.array: the optimized policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement Policy Iteration algorithm\n",
    "\n",
    "    policy = np.random.randint(0, 4, size=16)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test and analyze the algorithm\n",
    "\n",
    "dynamics = FrozenLakeMDP(is_slippery=False)\n",
    "policy = policy_iteration(dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test the policy on the environment\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "total_reward = record_video(env, policy, 'frozenlake_random.mp4', fps=5, random_action=False)\n",
    "print(f\"total reward: {total_reward}\")\n",
    "show_video('frozenlake_random.mp4', video_width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:  # The Q-Learning RL agent\n",
    "\n",
    "    def __init__(self, num_states, num_actions, epsilon, alpha, gamma=0.9, eps_end=0.01, eps_decay=3e-6):\n",
    "\n",
    "        self.num_states = num_states    # number of possible states\n",
    "        self.num_actions = num_actions  # number of possible actions\n",
    "        self.gamma = gamma              # discount factor\n",
    "        self.epsilon = epsilon          # initial exploration probability\n",
    "        self.alpha = alpha              # step size\n",
    "        self.eps_decay = eps_decay      # linear decay rate of epsilon\n",
    "        self.eps_end = eps_end          # minimum value for epsilon\n",
    "        self.q_table = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        chooses an action in an epsilon-greedy manner.\n",
    "    \n",
    "        Args:\n",
    "            state (int): current state of the agent.\n",
    "    \n",
    "        Returns:\n",
    "            int: the chosen action\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: implement epsilon-greedy action selection\n",
    "\n",
    "        action = 0\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        updates the q-table based on a single interaction with the environment.\n",
    "    \n",
    "        Args:\n",
    "            state (int): state of the agent.\n",
    "            action (int): action chosen by the agent.\n",
    "            reward (int): reward obtained by the agent.\n",
    "            next_state (int): next state of the agent.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement Q-table update\n",
    "\n",
    "        # epsilon decay\n",
    "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_end else self.eps_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=100000):\n",
    "    \"\"\"\n",
    "        trains an agent through interactions with the environemnt using Q-learning.\n",
    "    \n",
    "        Args:\n",
    "            env (gym.Env): the gym environment.\n",
    "            agent (QAgent): the Q-learning agent.\n",
    "            n_episodes (int): number of training episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        # TODO: implement the training loop for Q-learning\n",
    "\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "agent = QAgent(num_states=16, num_actions=4, epsilon=1.0, alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: obtain the policy by a simple argmax on agent's Q-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test the policy on the environment\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "total_reward = record_video(env, policy, 'frozenlake_random.mp4', fps=5, random_action=False)\n",
    "print(f\"total reward: {total_reward}\")\n",
    "show_video('frozenlake_random.mp4', video_width=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
